{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1a6843-f3fd-41c6-a35e-1dff204871d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: azure_replication_plan.md\n",
      "✅ Created: data_lake_structure.md\n",
      "✅ Created: delta_table_design.md\n",
      "✅ Created: notebook_migration_log.md\n",
      "✅ Created: performance_and_cost_notes.md\n",
      "✅ Created: databricks_jobs_schedule.md\n",
      "✅ Created: open_questions_and_decisions.md\n",
      "\n",
      "📁 All files saved to: C:\\Users\\mikek\\One Drive New\\OneDrive\\Leviathan\\Leviathan for Azure ideas\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\mikek\\One Drive New\\OneDrive\\Leviathan\\Leviathan for Azure ideas\\scripts\")\n",
    "\n",
    "\n",
    "# Root directory\n",
    "azure_root = Path(r\"C:\\Users\\mikek\\One Drive New\\OneDrive\\Leviathan\\Leviathan for Azure ideas\")\n",
    "azure_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Main overview markdown content\n",
    "overview_md = \"\"\"# ⚓ Leviathan Azure Replication Plan\n",
    "\n",
    "This document outlines how the Leviathan football modeling project will be migrated and enhanced on the Azure + Databricks platform using a modern Data Lake and Delta Lake architecture.\n",
    "\n",
    "See related markdown files in this folder for full detail on table design, data layers, and notebook migration.\n",
    "\"\"\"\n",
    "\n",
    "# File names and optional content\n",
    "markdown_files = {\n",
    "    \"azure_replication_plan.md\": overview_md,\n",
    "    \"data_lake_structure.md\": \"# 🗂️ Data Lake Folder Structure\\n\\nDefine lake zones: raw, curated, feature/model.\",\n",
    "    \"delta_table_design.md\": \"# 📐 Delta Table Design\\n\\nList tables, expected schema, partitioning, update cadence.\",\n",
    "    \"notebook_migration_log.md\": \"# 📓 Notebook Migration Log\\n\\nTrack which scripts are ported to Databricks.\",\n",
    "    \"performance_and_cost_notes.md\": \"# 💸 Performance & Cost Considerations\\n\\nMemory use, compute configs, storage tiers.\",\n",
    "    \"databricks_jobs_schedule.md\": \"# ⏰ Databricks Jobs & Scheduling\\n\\nDefine pipelines, jobs, triggers, retry logic.\",\n",
    "    \"open_questions_and_decisions.md\": \"# ❓ Open Questions & Decisions\\n\\nDocument unresolved issues and key tradeoffs.\",\n",
    "}\n",
    "\n",
    "# Create files\n",
    "for fname, content in markdown_files.items():\n",
    "    fpath = azure_root / fname\n",
    "    fpath.write_text(content.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"✅ Created: {fpath.name}\")\n",
    "\n",
    "print(f\"\\n📁 All files saved to: {azure_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e693a02e-bbeb-4429-8bdd-deb0ce3fa558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: C:\\Users\\mikek\\One Drive New\\OneDrive\\Leviathan\\Leviathan for Azure ideas\\azure_replication_plan.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define target folder\n",
    "azure_path = Path(r\"C:\\Users\\mikek\\One Drive New\\OneDrive\\Leviathan\\Leviathan for Azure ideas\")\n",
    "azure_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define markdown content\n",
    "markdown = \"\"\"# ⚓ Leviathan Azure Replication Plan\n",
    "\n",
    "This document outlines how the Leviathan football modeling project will be migrated and enhanced on the Azure + Databricks platform using a modern Data Lake and Delta Lake architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 1. Data Lake Structure (Azure Data Lake Storage Gen2)\n",
    "\n",
    "abfss://leviathan@<storage_account>.dfs.core.windows.net/\n",
    "├── raw/\n",
    "│ └── england_1/\n",
    "│ └── plankton/ # Original match data (Plankton layer)\n",
    "├── curated/\n",
    "│ ├── ocean/\n",
    "│ │ └── england_1/\n",
    "│ │ ├── krill/\n",
    "│ │ ├── squid/\n",
    "│ │ ├── turtle/\n",
    "│ ├── armory/\n",
    "│ │ ├── crowsnest/ # Points system from plankton\n",
    "│ │ ├── whaleboat/ # Team form and rolling totals\n",
    "│ │ ├── tackle/ # Home advantage adjustments\n",
    "│ │ ├── line/\n",
    "│ │ └── harpoon/\n",
    "\n",
    "yaml\n",
    "Copy\n",
    "Edit\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ 2. Databricks Delta Lake Tables\n",
    "\n",
    "| Layer       | Delta Table                          | Description                              |\n",
    "|-------------|---------------------------------------|------------------------------------------|\n",
    "| Raw Match   | `raw.england_1.plankton`              | Basic match results                      |\n",
    "| Points      | `curated.armory.crowsnest_points`     | Points per team per match                |\n",
    "| Form        | `curated.armory.whaleboat_form`       | Cumulative and rolling team performance  |\n",
    "| Odds        | `curated.ocean.turtle`                | WH, VC, B365 odds                        |\n",
    "| Model Ready | `features.spermwhale_ready`           | Fully complete rows for training         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 3. Transformations & Scripts\n",
    "\n",
    "- Jupyter notebooks → Databricks Notebooks\n",
    "- Scripts → Jobs or scheduled workflows\n",
    "- CSV → Delta tables (via Auto Loader or ingestion pipeline)\n",
    "- Save all logic with versioned notebooks in GitHub\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 4. Improvements Over Local\n",
    "\n",
    "- No duplicated CSVs — derived layers are materialized or views\n",
    "- Faster queries via Delta format\n",
    "- Scalable to include leagues beyond England_1\n",
    "- Easier experimentation with MLflow model tracking\n",
    "\n",
    "---\n",
    "\n",
    "## 🔜 Next Steps\n",
    "\n",
    "- [ ] Create `Leviathan` container and lake folders in Azure\n",
    "- [ ] Upload Ocean files (e.g. Plankton CSVs) to raw zone\n",
    "- [ ] Define transformation notebooks for CrowsNest, Whaleboat, etc.\n",
    "- [ ] Register Delta tables in Unity Catalog or Hive Metastore\n",
    "- [ ] Begin modeling using Spark + MLflow\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "target_file = azure_path / \"azure_replication_plan.md\"\n",
    "target_file.write_text(markdown, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Saved: {target_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997e4b2-db2c-4845-a6d5-20ff3bd28eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996aaab-9361-4146-b70e-0d092da93c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b568d8b-d2d8-4718-844e-136179d3e8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
